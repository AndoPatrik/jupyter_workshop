# Data, Models, and Errors - with solutions

In this notebook we will look at the most simple machine learning model, namely *linear regression*. Eventhough it is simple, linear regression and its many exetions are still powerful and should always serve as a baseline model: If you try build a machine learning model for a problem where you can use linear regression, whatever model you build you should always compare its preformance to linear regression.

This notebook also serve another purpose. It will get you more familiar with python and some of the most central packages and data structures we are going to use in this course.
## Getting the environment right
Make sure you have imported and selected the relevant environment file, AI_Keras_env_MAC.yml or AI_Keras_env_windows.yml
### Importing data

We will start from the very beginning, where most machine learning model build starts, namely by reading in some data. In this case a csv file. The easiest way to read in data for data analysis is with the panda package. So we will import the pandas package and read in the csv file 'potatoprises.csv'.
pip install pandas

data_directory = 'shared/DeepLearningClass/Week1/potatoprices.csv'

import pandas as pd
potato_prices = pd.read_csv(data_directory + 'potatoprices.csv')
Pandas is a heavily used package for doing all sort of data analysis i Python. For those familiar with R, pandas allow us to use data structures that resemble R's data.frame data strux
ture. For more about pandas see: https://pandas.pydata.org/
Have a look at the data:
potato_prices
Let us now plot the data. Actually, as we loaded in the data using the pandas package, our data is in a pandas data frame type object (try to run the code `type(potato_prices)`), which means that there is alread plotting abilities in the object. So we could just do:
potato_prices.plot()
(For some reason, still unknown to me, the plot does not always show up the first time. If that is the case, just execute the cell again.)

To make the plots bigger in Jupyter Notebook such that they are easier to see we can use the following line of code. (You do not need to understand this now.)
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [10, 6]
potato_prices.plot()
**Question:** What is the x-axis and y-axis representing here? 

*Answer: The y-axis is the value (for the variables 'kg' and 'price', while the x-axis is simply the index (row number) of each data point. Unless we deal with data in some chronological order, this is not very useful.*
As your goal is to train a model that can predict the price of an amount of potatoes, we are interested in the relationship between the two variables (or features), *kg* and *price*. The better see such a potential relationship, we usually use a scatterplot. To do this, we can just use the following code:
potato_prices.plot.scatter(x = 'kg', y = 'price')
### Simple linear regression

Clearly there is a linear relationship between $kg$ and $price$. A model that models that relationship will be a straight line in the 'kg'-'price' plane and can be represented by an equation of the form

$$
price = f(kg) = a * kg + b , 
$$

In other words, the variable $price$ is a function of the variable $kg$. In this case we call $price$ the *response variable* or the *dependent variable*. $kg$ we call the *independent variable*, a *feature* or a *predictor variable*. In the above equation $a$ and $b$ are constant values we do not know. Each possible instance of $a$ and $b$ will give rise to one particular (different) model. Thus, we also refers to $a$ and $b$ as the parameters of the model. In the particular case of linear regression a parameter like $a$ is also refered to as a *coefficient* and $b$ the *intercept*.

The learning part of machine learning, often also refered to as the fitting process, is about comming up with good (or the best) values for the parameters of the model (in this case $a$ and $b$).

Let us try out a few possible such models (i.e. values for $a$ and $b$). Try to understand what the code below does and how we plot the different models. (For now ignore the code line `import matplotlib.pyplot as plt`.)
x = potato_prices['kg']
y = potato_prices['price']
model_1 = 0.6 * x + 0.4 # model_1 = y_pred_1 (or "y-hat_1")
model_2 = 0.5 * x + 0.4
model_3 = 0.4 * x + 0.6
model_4 = 0.5 * x + 0.5

import matplotlib.pyplot as plt
potato_prices.plot.scatter(x = 'kg', y = 'price')
plt.plot(x, model_1, 'red')
plt.plot(x, model_2, 'orange')
plt.plot(x, model_3, 'cyan')
plt.plot(x, model_4, 'purple')
**Question:** Which model seems to be the best and what is the values of $a$ and $b$ in this case? Do you think it is possible to find a better model? 

*Answer: The purple line, model 4, seems to represent the best model here, in the sense that it seems to fit the data best. However, it might just be a lucky guess and there is no reason to assume that the purple model is the best possible model - we need an algorithm/solution formula to tell how to find the best model. (Again, we need to agree on precisely what we mean by "best model".)*
Linear regression is a method for finding the best such linear model. (What "best" means here, we will get back to.) One advantage of linear regression is that there is a closed form solution. That is, given a dataset, there is a formula that tells us how to find all the parameters for the best linear model. This is far from true for most machine learning models. Therefore, we will later discuss more general methods for finding good models. (Finding a good model that fits well to data is also refered to as *fitting*.) Below is an implementation of closed form solution for linear regression. Have a look at it and how we plot it.
import numpy as np

## The following code is based on well-known formulas you can find in any book on data science or on Wikipedia
sxy = 0
sxx = 0
for i in range(x.shape[0]):
    sxy += (x[i] - np.mean(x)) * (y[i] - np.mean(y))
    sxx += (x[i] - np.mean(x)) * (x[i] - np.mean(x))
    
a_linreg = sxy / sxx
b_linreg = np.mean(y) - a_linreg * np.mean(x)

print("a_linreg:", a_linreg)
print("b_linreg:", b_linreg)

y_pred = a_linreg * x + b_linreg
**Exercise:** Plot the data as well as this new linear regression model, just as we plotted the four models above.
potato_prices.plot.scatter(x = 'kg', y = 'price')
plt.plot(x, y_pred, 'limegreen')
**Question:** According to this linear regression model, what is the price for 0.8kg of potatoes?
# Answer:
a_linreg * 0.8 + b_linreg
### Measuring model error

There are several ways of measuring how good a model is. For regression tasks, like our potato price case here, where we are trying to predict a continous variable (price), a natural measure of the *goodness of fit* of a model is the model error.

As explained in the note "A gentle introduction to machine learning" by Henning Christiansen, we can measure the error of a model using the following formula

$$
err = \sum_{i=1}^n (y_i - f_{predict}(x_i))^2 ,
$$

where $\{ \langle x_i , y_i \rangle | 1 \leq i \leq n \}$ is our data set. So in our case, this formula takes for each data point and calculates the difference between the true price ($y_i$) and the predicted price ($y^{pred}_i$ = $f_{predict}(x_i)$), square this difference and finally sum up all these squared differences. The (non-squared) differences are represented as the orange lines in the plot below.
potato_prices.plot.scatter(x = 'kg', y = 'price')
plt.plot(x, y_pred, 'limegreen')
plt.vlines(x, y, y_pred, colors = "orange")
Note that the differences between the actual values and the predicted values ($y_i - y^{pred}_i$) are also refered to as the *residuals* of the model.

In the formula above we sum over all data points (of which we have $n$), so the more data we use to fit the model, the higher this error might become. Therefore, it is often more natural to look at the *average* error. Taking the average of the above formula give us the following error formula also refered to as *mean squared error (MSE)*:

$$
MSE = \frac{1}{n}\sum_{i=1}^n (y_i - f_{predict}(x_i))^2 .
$$

Sometimes you will also meet the *root mean squared error (RMSE)*, which is obtained by taking the square root of the above formula:

$$
RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - f_{predict}(x_i))^2} .
$$
**Exercise:** Write a python function that calculate the mean squared error of a model. Calculate the MSE of the linear regression model above. Afterwards also calculate the MSE of the four original models we plotted in the beginning. Below is the skeleton for a python function that does it. You just have to fill in some code instead of the dots.
# Answer:
def my_mse(actual_y, predicted_y):
    ...

print("The MSE of the linear regression model is:", my_mse(y, y_pred))

print("The MSE of the original model_1 is:", my_mse(y, 0.6 * x + 0.4))
print("The MSE of the original model_2 is:", my_mse(y, 0.5 * x + 0.4))
print("The MSE of the original model_3 is:", my_mse(y, 0.4 * x + 0.6))
print("The MSE of the original model_4 is:", my_mse(y, 0.5 * x + 0.5))
Hopefully you will see that the linear regression model indeed have the lowest error.
### Getting help from Python packages

Above we used the package pandas to help us import a data set as well as doing some plotting. As you might have noticed, we also used two other packages a bit, namely *matplotlib* and *numpy*. Matplotlib is the go-to package for plotting and numpy is the go-to package for numerical calculation. In this section we will look a little closer on these packages and add another one called *scikit-learn* (or *sklearn*). Together these four packages (pandas, numpy, matplotlib, and scikit-learn) make up the most used packages in any data scientist's python toolbox, so remeber them!
#### Numpy

The [numpy](http://www.numpy.org/) package is important as the numpy array is the data structure that all our data will be in when we will train neural network - the book *Deep Learning with Python by Francios Chollet* also refers to these multidimensional arrays as *tensors*. Moreover, the popular machine learning package scikit-learn also takes input data numpy array format. Note that a numpy array is something else than the standard array type of Python. It is recomended to have a closer look at Numpy, for instance this quickstart tutorial: https://docs.scipy.org/doc/numpy/user/quickstart.html 
Numpy arrays can hold numeric data in 1-dimensional objects (vectors), 2-dimensional object (matrices), 3-dimensional objects, ... etc. In 2.2.8 on page 35 of the book [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) there are examples of how up to 5-dimensional tensors easily can occur with real data. Dimensions in numpy arrays are also refered to as axes.

Below is an example of a 1D, 2D and 3D array and how to construct such with numpy:
import numpy as np

x_1D = np.array([1,2,3,4,17,42])
print("x_1D:")
print(x_1D)
print("-------------")

x_2D = np.array([[2, 4, 6, 8],
                [1,3,5,7],
                [42,43, 44, 45]])
print("x_2D:")
print(x_2D)
print("-------------")

x_3D = np.array([[[2, 4, 6],
                [6, 8, 10],
                [10, 12, 14]],
                 [[1,3,5],
                 [5,7,9],
                 [9,11,13]]])
print("x_3D:")
print(x_3D)
Here `x_1D` is a 1-dimensional numpy array (a vector) of lenght 6 (i.e. it has one axis of length 6), `x_2D` a 2-dimensional numpy array with 3 rows and 4 columns (i.e. it has two axes of length 3 and 4), and `x_3D` is a 3-dimensional numpy array with 3 axes of length 2, 3, and 3.
**Exercise:** Look at the code for `x_1D`, `x_2D`, `x_3D` and make sure you understand how the sentence above is true. Then verify the sentence by calling the attributes `ndim` and `shape` on the numpy array objects. (See https://docs.scipy.org/doc/numpy/user/quickstart.html if you get stuck)
# Answer:

*Remember the quickstart tutorial: https://docs.scipy.org/doc/numpy/user/quickstart.html*
## Scikit-learn

God oldfashioned and well-tested machine-learning methods such as linear regression with multidimensional data are available in the [scikit-learn](https://scikit-learn.org/stable/).
However, we will not use such methods in the course, so we do not spend more time on that here.
